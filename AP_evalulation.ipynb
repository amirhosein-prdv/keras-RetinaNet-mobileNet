{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "from keras_retinanet.utils.eval import evaluate\n",
    "from keras_retinanet.utils.gpu import setup_gpu\n",
    "from keras_retinanet.models.mobilenet import custom_objects\n",
    "setup_gpu('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32, numpy=\n",
      "array([[-22.627417, -11.313708,  22.627417,  11.313708],\n",
      "       [-28.50876 , -14.25438 ,  28.50876 ,  14.25438 ],\n",
      "       [-35.918785, -17.959393,  35.918785,  17.959393],\n",
      "       [-16.      , -16.      ,  16.      ,  16.      ],\n",
      "       [-20.158737, -20.158737,  20.158737,  20.158737],\n",
      "       [-25.398417, -25.398417,  25.398417,  25.398417],\n",
      "       [-11.313708, -22.627417,  11.313708,  22.627417],\n",
      "       [-14.25438 , -28.50876 ,  14.25438 ,  28.50876 ],\n",
      "       [-17.959393, -35.918785,  17.959393,  35.918785]], dtype=float32)> anchors\n",
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32, numpy=\n",
      "array([[-45.254833, -22.627417,  45.254833,  22.627417],\n",
      "       [-57.01752 , -28.50876 ,  57.01752 ,  28.50876 ],\n",
      "       [-71.83757 , -35.918785,  71.83757 ,  35.918785],\n",
      "       [-32.      , -32.      ,  32.      ,  32.      ],\n",
      "       [-40.317474, -40.317474,  40.317474,  40.317474],\n",
      "       [-50.796833, -50.796833,  50.796833,  50.796833],\n",
      "       [-22.627417, -45.254833,  22.627417,  45.254833],\n",
      "       [-28.50876 , -57.01752 ,  28.50876 ,  57.01752 ],\n",
      "       [-35.918785, -71.83757 ,  35.918785,  71.83757 ]], dtype=float32)> anchors\n",
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32, numpy=\n",
      "array([[ -90.50967 ,  -45.254833,   90.50967 ,   45.254833],\n",
      "       [-114.03504 ,  -57.01752 ,  114.03504 ,   57.01752 ],\n",
      "       [-143.67514 ,  -71.83757 ,  143.67514 ,   71.83757 ],\n",
      "       [ -64.      ,  -64.      ,   64.      ,   64.      ],\n",
      "       [ -80.63495 ,  -80.63495 ,   80.63495 ,   80.63495 ],\n",
      "       [-101.593666, -101.593666,  101.593666,  101.593666],\n",
      "       [ -45.254833,  -90.50967 ,   45.254833,   90.50967 ],\n",
      "       [ -57.01752 , -114.03504 ,   57.01752 ,  114.03504 ],\n",
      "       [ -71.83757 , -143.67514 ,   71.83757 ,  143.67514 ]],\n",
      "      dtype=float32)> anchors\n",
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32, numpy=\n",
      "array([[-181.01933,  -90.50967,  181.01933,   90.50967],\n",
      "       [-228.07008, -114.03504,  228.07008,  114.03504],\n",
      "       [-287.35028, -143.67514,  287.35028,  143.67514],\n",
      "       [-128.     , -128.     ,  128.     ,  128.     ],\n",
      "       [-161.2699 , -161.2699 ,  161.2699 ,  161.2699 ],\n",
      "       [-203.18733, -203.18733,  203.18733,  203.18733],\n",
      "       [ -90.50967, -181.01933,   90.50967,  181.01933],\n",
      "       [-114.03504, -228.07008,  114.03504,  228.07008],\n",
      "       [-143.67514, -287.35028,  143.67514,  287.35028]], dtype=float32)> anchors\n",
      "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32, numpy=\n",
      "array([[-362.03867, -181.01933,  362.03867,  181.01933],\n",
      "       [-456.14017, -228.07008,  456.14017,  228.07008],\n",
      "       [-574.70056, -287.35028,  574.70056,  287.35028],\n",
      "       [-256.     , -256.     ,  256.     ,  256.     ],\n",
      "       [-322.5398 , -322.5398 ,  322.5398 ,  322.5398 ],\n",
      "       [-406.37466, -406.37466,  406.37466,  406.37466],\n",
      "       [-181.01933, -362.03867,  181.01933,  362.03867],\n",
      "       [-228.07008, -456.14017,  228.07008,  456.14017],\n",
      "       [-287.35028, -574.70056,  287.35028,  574.70056]], dtype=float32)> anchors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.pourdavoud\\Anaconda3\\envs\\example\\lib\\site-packages\\keras\\engine\\training_utils.py:819: UserWarning: Output non_maximum_suppression_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to non_maximum_suppression_2.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "classes = \"D:\\Amirhosein\\Object_Detection\\\\tag-detection-RetinaNet\\\\tag-detection-retinanet_MobileNet\\dataset\\\\classes.csv\"\n",
    "val_path = \"D:\\Amirhosein\\Object_Detection\\\\tag-detection-RetinaNet\\\\tag-detection-retinanet_MobileNet\\dataset\\\\val.csv\"\n",
    "test_path = \"D:\\Amirhosein\\Object_Detection\\\\tag-detection-RetinaNet\\\\tag-detection-retinanet_MobileNet\\dataset\\\\test.csv\"\n",
    "model_path = \"D:\\Amirhosein\\Object_Detection\\\\tag-detection-RetinaNet\\\\tag-detection-retinanet_MobileNet\\\\snapshots\\mobilenet_a1_s8_rdc2\\mobilenet_a1_s8_rdc2_best.h5\"\n",
    "# model_path = \"D:\\Amirhosein\\Object_Detection\\keras-retinanet\\snapshots\\retinanet_resnet_09_2.h5\"\n",
    "\n",
    "test_image_data_generator = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "    # create a generator for testing data\n",
    "test_generator = CSVGenerator(\n",
    "    csv_data_file=test_path,\n",
    "    csv_class_file=classes,\n",
    "    image_data_generator=test_image_data_generator,\n",
    "    batch_size=2,\n",
    "    )\n",
    "\n",
    "model = load_model(model_path, custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a Trackable object for export, got <keras.engine.training.Model object at 0x00000249E7B0A548>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12316\\341428051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\a.pourdavoud\\Anaconda3\\envs\\example\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    878\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrackable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m     raise ValueError(\n\u001b[1;32m--> 880\u001b[1;33m         \"Expected a Trackable object for export, got {}.\".format(obj))\n\u001b[0m\u001b[0;32m    881\u001b[0m   \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msave_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a Trackable object for export, got <keras.engine.training.Model object at 0x00000249E7B0A548>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "tms = tf.saved_model.save(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: : 100%|██████████| 88/88 [00:18<00:00,  4.86it/s]\n",
      "Parsing annotations: : 100%|██████████| 88/88 [00:00<00:00, 682.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 instances of class tag with:\n",
      "recall_of_tag: 0.8295\n",
      "precision_of_tag: 0.8391\n",
      "accuracy: 0.8295\n",
      "average_precision_of_tag: 0.7425\n",
      "inference_time: 0.1351\n",
      "mAP: 0.7425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run evaluation\n",
    "logs = evaluate(\n",
    "    generator=test_generator,\n",
    "    model=model,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.05,\n",
    "    max_detections=100,\n",
    "    save_path='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_path, header=None )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = df.iloc[:][0]\n",
    "img_test = img_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_search_retinanet(img, output_path):\n",
    "        scale = 1.0\n",
    "        bb_list = []\n",
    "        label_list= [0]\n",
    "        labels_to_names = {0:'tag'}\n",
    "        image_path = img\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "        img_scaled = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        img_scaled = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "\n",
    "        start = time.time()\n",
    "        _, _, detections = model.predict_on_batch(np.expand_dims(img_scaled, axis=0))\n",
    "        inference_time = time.time() - start\n",
    "        \n",
    "        predicted_labels = np.argmax(detections[0, :, 4:], axis=1)\n",
    "        scores = detections[0, np.arange(detections.shape[1]), 4 + predicted_labels]\n",
    "        detections[:, :4] /= scale\n",
    "\n",
    "        for idx, (label, score) in enumerate(zip(predicted_labels, scores)):\n",
    "            if score < 0.9:\n",
    "                continue\n",
    "            if label not in label_list:\n",
    "                continue\n",
    "            b = detections[0, idx, :4].astype(int)\n",
    "            bb_list.append([(b[0], b[1], b[2], b[3]), score])\n",
    "        \n",
    "        # selected_indices = tf.image.non_max_suppression([i for i, _ in bb_list], [i for _, i in bb_list], max_output_size=2, iou_threshold=0.5)\n",
    "        # selected_boxes = tf.gather([i for i, _ in bb_list], selected_indices)\n",
    "        # print(selected_boxes.shape)\n",
    "        # draw_boxes(img, selected_boxes, color=(100, 100, 250))\n",
    "        for b, score in bb_list:\n",
    "            draw_box(img, b, color=(100, 100, 250))\n",
    "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            draw_caption(img, b, caption)\n",
    "\n",
    "        img_name = image_path.split('\\\\')[-1]\n",
    "        output_path = output_path + f'/output/out_{img_name}'\n",
    "        cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm as t\n",
    "labels_to_names = {0: 'tag'}\n",
    "output_path = \"D:\\Amirhosein\\Object_Detection\\keras-retinanet-mobilenet\"\n",
    "for image_path in t(img_test[3:4]):\n",
    "    tag_search_retinanet(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('example')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c13b3c941718669a924020a6383c807508bfb8ef6ea829f0ec88eba357603a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
